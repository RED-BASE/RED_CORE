{\rtf1\ansi\ansicpg1252\cocoartf2822
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 Dear 80,000 Hours Team,\
\
  5 weeks ago (late April) I caught the AI safety and\
   red teaming bug. Since then I started teaching \
  myself Python from zero (with a lot of help from \
  AI) to build a systematic AI safety research \
  framework called RED_CORE.\
\
  I'm honestly obsessed and would love to do this \
  sort of work for a living BUT I currently work at a\
   pet food store and I didn't go to college. I'm not\
   sure if I'm heading in the right direction or what\
   gaps I need to fill to be competitive for AI \
  safety roles.\
\
  That's really why I'm writing. I could use some \
  critical feedback on what I've built so far, \
  whether it has a place in the professional AI \
  safety world, and if not, what it would take to get\
   there.\
\
  I have attached a few example logs for review. The \
  full repo is private but available upon request.\
\
  Warm regards,\
\
  Casius Oldenburg} Perfect ‚Äî thanks for the clarification. Since both options are technical, let‚Äôs frame them as two distinct tracks within the LLM red teaming landscape: one independent and tool-focused (builder path), and one embedded and evaluative (analyst/operator path). This will highlight both your versatility and strategic thinking while keeping everything rooted in what you‚Äôre already doing.

Here‚Äôs the revised version with clarity between the two paths:

‚∏ª

üß∞ Option 1: Red Team Infrastructure Builder / Independent Evaluator

Summary:
Continue developing RED_CORE into a robust open-source evaluation framework. Use this to demonstrate technical initiative and systems design skill ‚Äî potentially transitioning into a tooling, test harness, or research engineer role focused on red teaming infrastructure.

Pros:
	‚Ä¢	Leverages my current momentum: I‚Äôve already built and refined a working multi-model test harness
	‚Ä¢	Aligns with my passion for building structured systems to explore AI behavior
	‚Ä¢	Lets me define a niche contribution ‚Äî many orgs lack reliable, open red team tooling
	‚Ä¢	Strong portfolio potential; can prove my skill before applying

Cons:
	‚Ä¢	Risk of building in isolation ‚Äî limited mentorship or feedback loop outside of public contributions
	‚Ä¢	Requires deeper engineering knowledge over time (APIs, infra, security, versioning)
	‚Ä¢	Could slow down my path to employment unless I secure visibility or collaborators
	‚Ä¢	Harder to know when it‚Äôs ‚Äúgood enough‚Äù to apply with

‚∏ª

üî¨ Option 2: LLM Red Team Analyst / Evaluation Contributor (Embedded Track)

Summary:
Position myself for a role on an evals team at an AI lab or safety org. Focus on stress-testing LLMs for failure modes using prompts, scenarios, and interpretive reasoning ‚Äî with less emphasis on tooling, more on attack creativity and analytical skill.

Pros:
	‚Ä¢	Closer to existing roles (ARC, Apollo, OpenAI) ‚Äî better-defined hiring pipelines
	‚Ä¢	Requires less independent infra-building ‚Äî can focus on generating high-signal attacks and evaluations
	‚Ä¢	My strengths in linguistic play, adversarial testing, and conceptual framing are a good fit
	‚Ä¢	Easier to grow within a team and receive mentorship

Cons:
	‚Ä¢	May be more competitive without formal credentials or prior lab affiliation
	‚Ä¢	Some orgs may prioritize experienced researchers or CS grads
	‚Ä¢	Fewer ‚Äúportfolio artifacts‚Äù if I‚Äôm not publishing tools or results
	‚Ä¢	Less autonomy or creative control compared to an independent builder path

‚∏ª

üß≠ Tentative Leaning:

I‚Äôm drawn to both paths. Option 1 gives me immediate traction and creative autonomy, but I‚Äôm aware of its longer solo runway. Option 2 feels like a faster on-ramp to collaborative impact and mentorship. My current approach is to treat RED_CORE as both a proving ground and a signal ‚Äî while staying open to joining an evals team if a good fit emerges.

‚∏ª

Want me to turn this into a polished version for direct inclusion in your email or form? Or add a brief note about how this fits your long-term goals (e.g., contributing to AI alignment, interpretability, or testing infrastructure)?


