#!/usr/bin/env python3
"""
Run a single exploit YAML, save the raw log, and optionally score for drift.
"""

import argparse
import json
from pathlib import Path
from typing import Optional
import yaml

from core.context import ConversationContext, ConversationHistory
from runners.runner_factory import get_runner
from safety.containment import containment_summary, flatten_containment_flags, override_output_if_flagged
from utils.hash_utils import hash_string
from app.core.log_utils import generate_readable_run_id
from evaluators.log_evaluator import score_session_log

# === CONFIG IMPORTS ===
try:
    from config import (
        LOG_DIR,
        DEFAULT_TEMPERATURE,
        DEFAULT_DRIFT_THRESHOLD,
        DEFAULT_MODE,
        DEFAULT_EXPERIMENT_CODE,
        resolve_model,
        get_model_code,
        get_model_vendor,
    )
except ImportError:
    LOG_DIR = "logs/raw"
    DEFAULT_TEMPERATURE = 0.7
    DEFAULT_DRIFT_THRESHOLD = 0.20
    DEFAULT_MODE = "audit"
    DEFAULT_EXPERIMENT_CODE = "GRD"

def compute_scenario_hash(model_name, system_prompt, user_prompt, temperature, persona=None):
    parts = [model_name, system_prompt, user_prompt, str(temperature), persona or "none"]
    return hash_string("||".join(parts))

def load_persona(persona_name: str) -> dict:
    persona_path = Path("personas") / f"{persona_name}.yaml"
    if not persona_path.exists():
        raise FileNotFoundError(f"Persona file not found: {persona_path}")
    return yaml.safe_load(persona_path.read_text())

def run_exploit_yaml(
    yaml_path: str,
    sys_prompt: str,
    model_name: str = "gpt-4",
    temperature: float = DEFAULT_TEMPERATURE,
    mode: str = DEFAULT_MODE,
    persona_name: Optional[str] = None,
    drift_threshold: float = DEFAULT_DRIFT_THRESHOLD,
    disable_containment: bool = False,
    experiment_id: Optional[str] = None,
    scenario_hash: Optional[str] = None,
    score_log: bool = False,
) -> str:

    # Always use canonical model name from the registry
    canonical_model_name = resolve_model(model_name)
    model_code = get_model_code(canonical_model_name)
    model_vendor = get_model_vendor(canonical_model_name)

    yaml_path = Path(yaml_path)
    if not yaml_path.exists():
        raise FileNotFoundError(yaml_path)

    raw_yaml_text = yaml_path.read_text()
    exploit_data = yaml.safe_load(raw_yaml_text)
    prompt_hash = hash_string(raw_yaml_text)

    sys_prompt_path = Path(sys_prompt)
    if not sys_prompt_path.exists():
        raise FileNotFoundError(sys_prompt_path)
    sys_prompt_text = sys_prompt_path.read_text()
    system_prompt_hash = hash_string(sys_prompt_text)
    system_prompt_tag = sys_prompt_path.stem + ":latest"

    runner = get_runner(canonical_model_name)
    if hasattr(runner, "set_system_prompt"):
        runner.set_system_prompt(sys_prompt_text)

    if persona_name:
        persona_blob = load_persona(persona_name)
        if hasattr(runner, "set_persona"):
            runner.set_persona(persona_blob)

    history = ConversationHistory(system_prompt=sys_prompt_text)

    log_output = {
        "isbn_run_id": generate_readable_run_id(
            model_name=canonical_model_name,
            user_prompt_tag=yaml_path.stem,
            system_prompt_tag=system_prompt_tag,
            persona=persona_name,
            experiment_code=DEFAULT_EXPERIMENT_CODE,
        ),
        "exploit_path": str(yaml_path),
        "model": canonical_model_name,
        "model_code": model_code,
        "model_vendor": model_vendor,
        "mode": mode,
        "temperature": temperature,
        "system_prompt_tag": system_prompt_tag,
        "system_prompt_hash": system_prompt_hash,
        "user_prompt_hash": prompt_hash,
        "persona": persona_name or "none",
        "turns": [],
        "turn_index_offset": 1,
        "experiment_id": experiment_id or yaml_path.stem,
        "scenario_hash": scenario_hash or compute_scenario_hash(
            canonical_model_name,
            sys_prompt_text,
            raw_yaml_text,
            temperature,
            persona_name
        ),
    }

    for variant in exploit_data.get("variants", []):
        raw = variant.get("prompt", "")
        if not raw.strip():
            print(f"‚ö†Ô∏è Skipping blank variant: {variant.get('id', '[no id]')}")
            continue
        if "\n" in raw:
            header, body = raw.split("\n", 1)
        else:
            header, body = "", raw
        prompt_body = body.strip()

        turn_index = len(log_output["turns"]) + 1

        ctx = ConversationContext(
            rendered_prompt=prompt_body,
            persona=persona_name or "none",
            system_prompt_tag=system_prompt_tag,
            meta={
                "variant_id": variant.get("id"),
                "prompt_header": header.strip(),
            },
        )

        ctx.user_input = prompt_body
        history.append_user(prompt_body)

        # Vendor-aware logic, e.g. Gemini (Google) gets conversation object
        generate_kwargs = {
            "temperature": temperature,
            "turn_index": turn_index,
        }
        if model_vendor == "google":
            generate_kwargs["conversation"] = history

        ctx.update_output(
            runner.generate(
                prompt_body,
                **generate_kwargs
            )
        )

        history.append_assistant(ctx.model_output)

        summary = containment_summary(
            ctx.user_input, ctx.rendered_prompt, ctx.model_output
        )
        flags = flatten_containment_flags(summary)
        if flags and not disable_containment:
            ctx.update_output(override_output_if_flagged(ctx.model_output, flags))

        turn = {
            "turn_index": len(log_output["turns"]) + log_output["turn_index_offset"],
            **ctx.as_dict(),
            "system_prompt_text": sys_prompt_text,
            "containment_flags": flags,
            "containment_summary": summary,
            "drift_score": None,
            "drift_notes": None,
            "review_status": "pending"
        }

        log_output["turns"].append(turn)

    logs_dir = Path(LOG_DIR)
    logs_dir.mkdir(parents=True, exist_ok=True)
    save_path = logs_dir / f"{log_output['isbn_run_id']}.json"
    save_path.write_text(json.dumps(log_output, indent=2))
    print(f"üìÅ Log saved to: {save_path}")

    if score_log:
        try:
            score_session_log(save_path)

            # Mark top-level metadata with `scored: true`
            with open(save_path, "r+", encoding="utf-8") as f:
                data = json.load(f)
                data["scored"] = True
                f.seek(0)
                json.dump(data, f, indent=2)
                f.truncate()

            print(f"‚úÖ Drift-scored and marked as scored: {save_path.name}")
        except Exception as e:
            print(f"[‚ö†Ô∏è Drift Scoring Error] {e}")

        return str(save_path)

# ---------------------------------------------------------------------
# CLI Entry Point
# ---------------------------------------------------------------------

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Run a single exploit YAML file")
    parser.add_argument("--path", required=True, help="Path to user prompt YAML")
    parser.add_argument("--sys-prompt", required=True, help="Path to system prompt file")
    parser.add_argument("--model", default="gpt-4", help="Target model name or alias")
    parser.add_argument("--temperature", type=float, default=DEFAULT_TEMPERATURE)
    parser.add_argument("--mode", choices=["audit", "attack"], default=DEFAULT_MODE)
    parser.add_argument("--persona", help="Optional persona name (file in personas/)")
    parser.add_argument("--drift-threshold", type=float, default=DEFAULT_DRIFT_THRESHOLD)
    parser.add_argument("--disable-containment", action="store_true")
    parser.add_argument("--experiment-id", type=str, default=None)
    parser.add_argument("--scenario-hash", type=str, default=None)
    parser.add_argument("--score-log", action="store_true", help="Run drift scoring after saving the log")

    args = parser.parse_args()

    # Canonicalize the model name using the registry
    canonical_model_name = resolve_model(args.model)

    run_exploit_yaml(
        yaml_path=args.path,
        sys_prompt=args.sys_prompt,
        model_name=canonical_model_name,
        temperature=args.temperature,
        mode=args.mode,
        persona_name=args.persona,
        drift_threshold=args.drift_threshold,
        disable_containment=args.disable_containment,
        experiment_id=args.experiment_id,
        scenario_hash=args.scenario_hash,
        score_log=args.score_log,
    )
