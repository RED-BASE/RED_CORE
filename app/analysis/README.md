# analysis/

Scripts and tools for log analysis, summarization, and review.

## üéØ Quick Start

Use the unified analysis CLI for all analysis tasks:

```bash
# Run complete analysis for an experiment
poetry run python app/analysis/analysis_cli.py full-analysis --experiment refusal_robustness

# Quick insights from existing flat logs
poetry run python app/analysis/analysis_cli.py quick-insights

# Extract metadata from raw logs  
poetry run python app/analysis/analysis_cli.py extract-metadata --log-dir experiments/refusal_robustness/logs/

# Interactive review tool
poetry run python app/analysis/analysis_cli.py review --log-dir experiments/refusal_robustness/logs/

# Automated safety scoring (NEW!)
poetry run python app/analysis/analysis_cli.py score --log-dir experiments/refusal_robustness/logs/ --standard mlcommons_ailuminate

# Full analysis with automated scoring
poetry run python app/analysis/analysis_cli.py full-analysis --experiment refusal_robustness --include-scoring
```

## üìÅ Tools Overview

### `analysis_cli.py` - **Unified Interface** 
Coordinates all analysis tools through a single command-line interface. **Use this for all analysis tasks.**

### `log_insight_report.py` - **Quick Analytics**
Fast statistical overview of experiment results:
- Model performance comparisons
- Drift pattern analysis  
- Anomaly detection

### `analyze.py` - **Metadata Extraction**
Extracts structured metadata from raw experiment logs:
- Run summaries and statistics
- Alignment pattern detection
- System prompt drift analysis

### `roller.py` - **Data Aggregation**
Rolls up turn-level data into run-level and experiment-level summaries:
- Trajectory classification (stable, escalating, snap, backslide)
- Aggregate statistics
- Cross-experiment comparisons

### `enhanced_review_tool.py` - **Interactive Review**
Comprehensive manual review interface:
- Color-coded turn-by-turn analysis
- Interactive scoring and annotation
- Export capabilities for scored data

### `automated_scorer.py` - **Industry-Standard Automated Scoring** ‚≠ê
Automated safety evaluation using industry benchmarks:
- **MLCommons AILuminate** standard support
- Multi-pattern refusal detection (full/partial/none)
- Hazard category classification (12+ categories)
- Ensemble evaluation approach
- Batch processing for efficiency
- Confidence scoring and reasoning

## üîÑ Analysis Workflow

1. **Run Experiments** ‚Üí Raw JSON logs saved to `experiments/{name}/logs/`
2. **Extract Metadata** ‚Üí Structured summaries and statistics  
3. **Flatten Data** ‚Üí Turn-level CSV for detailed analysis
4. **Roll Up Data** ‚Üí Run-level and experiment-level aggregations
5. **Generate Insights** ‚Üí Quick statistical reports
6. **Manual Review** ‚Üí Interactive scoring via enhanced review tool
7. **Export Results** ‚Üí Scored logs to `experiments/{name}/scored_logs/`

## üìä Data Flow

```
Raw Logs (JSON) 
    ‚Üì extract-metadata
Summary Data (CSV)
    ‚Üì flatten (via core/flattener.py)  
Flat Logs (CSV)
    ‚Üì roll-up
Rolled Data (CSV)
    ‚Üì quick-insights + review
Analysis Reports + Scored Logs
``` 

