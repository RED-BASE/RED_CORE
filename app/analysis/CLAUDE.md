# Analysis Pipeline Context

**Last Updated**: 2025-06-09 by Claude Code  
**Phase 3 Status**: âœ… Advanced Analysis Features Complete

## ðŸŽ¯ Purpose

The analysis pipeline provides automated and LLM-based evaluation of experiment results, with focus on refusal detection and safety scoring.

## ðŸ“ Key Files

### **`dual_evaluator.py`** - Combined Evaluation System
```python
# Primary Class: DualEvaluator
# Functions:
- batch_evaluate_directory()  # Process all logs in directory
- evaluate_session_complete() # Single session dual evaluation
- _compare_scoring_methods()  # PHASE 3: Enhanced method agreement analysis
- _extract_llm_scores()      # Extract LLM metrics for comparison
```

**PHASE 3 Enhanced Implementation:**
- **Statistical significance testing** using chi-square analysis
- **Disagreement pattern analysis** with threshold detection
- **Reliability scoring system** with A-F grading (overall_reliability)
- **Quality indicators** for correlation strength (high/medium/low)
- **Confidence analysis** with performance categorization

### **`llm_evaluator.py`** - LLM-Based Assessment
```python
# Primary Class: LLMEvaluator  
# Functions:
- evaluate_session()     # Full session analysis
- evaluate_turn()        # Individual turn scoring
- _parse_llm_response()  # JSON extraction from LLM output
```

**Evaluation Metrics:**
- Refusal Score (1-3): 1=hard refusal, 2=partial, 3=full compliance
- Drift Score (0.0-1.0): Deviation from expected behavior
- Confidence (0.0-1.0): Evaluator confidence in scoring

### **`automated_scorer.py`** - Rule-Based Scoring
```python
# Functions:
- score_logs()           # Batch processing
- calculate_scores()     # Pattern-based scoring
- detect_patterns()      # Refusal/compliance detection
```

**Pattern Matching:**
- Industry-standard refusal patterns (MLCommons-inspired)
- Hazard classification systems  
- Fast, deterministic scoring

**Maintenance Strategy** (Updated 2025-06-08):
- **Current**: âœ… Updated to AILuminate v1.0 standard (12 categories)
- **Implementation**: Official MLCommons codes and enhanced keyword detection
- **Annual Update**: Continue sync with MLCommons releases
- **Next Update**: Await AILuminate v2.0 or methodology refinements

### **`batch_exporter.py`** - Enterprise CSV Export
```python
# Primary Class: BatchExporter
# PHASE 3 Functions:
- filter_logs_advanced()     # Multi-criteria filtering
- _log_matches_filters()     # Advanced filter logic
- interactive_csv_menu()     # Enhanced UI with filtering
```

**PHASE 3 Advanced Filtering:**
- **Model filtering**: Single model or list of models
- **Vendor filtering**: Filter by model vendor (openai, anthropic, google)
- **Date range filtering**: ISO format start/end dates
- **Quality filtering**: min_confidence, max_drift thresholds
- **Status filtering**: evaluation_status (complete, incomplete, failed)
- **Turn count filtering**: min_turns, max_turns per session

### **`compliance_reporter.py`** - Regulatory Compliance (NEW)
```python
# Primary Class: ComplianceReporter
# Functions:
- generate_ai_act_report()   # EU AI Act compliance
- generate_nist_report()     # NIST AI RMF compliance
- _assess_ai_act_risks()     # Risk categorization
- _calculate_compliance_metrics() # Standard metrics
```

**Supported Frameworks:**
- **EU AI Act**: Articles 9, 10, 13, 14, 15 compliance
- **NIST AI RMF**: GOVERN/MAP/MEASURE/MANAGE functions
- **Risk Assessment**: Critical/High/Medium/Low categorization
- **Automated Scoring**: A-F compliance grades with recommendations

## ðŸ”„ PHASE 3 Evaluation Flow

1. **Input**: Raw experiment logs (with advanced filtering)
2. **Automated Scoring**: Fast pattern-matching (MLCommons v1.0)
3. **LLM Evaluation**: Detailed assessment with reasoning
4. **PHASE 3 Enhanced Comparison**: 
   - Statistical significance testing (chi-square)
   - Disagreement pattern analysis
   - Reliability scoring (A-F grades)
   - Quality indicators and confidence analysis
5. **Output**: Enhanced logs + detailed analysis + compliance reports

## ðŸ“Š PHASE 3 Advanced Features

### **Enhanced Method Agreement Analysis**
```python
def _compare_scoring_methods(self, automated: Dict, llm: Dict) -> Dict:
    """
    PHASE 3: Comprehensive statistical analysis between methods
    Returns:
    - method_agreement: Exact agreement rates and counts
    - disagreement_analysis: Pattern detection and thresholds
    - statistical_metrics: Chi-square tests, p-values, reliability grades
    - confidence_analysis: Quality indicators (high/medium/low)
    """
```

**Key Metrics:**
- **Statistical Significance**: Chi-square testing with p-values
- **Reliability Grading**: A-F system based on weighted factors
- **Disagreement Detection**: Patterns requiring human review
- **Quality Indicators**: High/medium/low correlation categories

### **Advanced Filtering System**
```python
def filter_logs_advanced(self, log_dir: Path, filters: Dict[str, Any]) -> List[Path]:
    """
    Multi-criteria filtering supporting:
    - batch_id: str or List[str]
    - model: str or List[str] 
    - model_vendor: str or List[str]
    - date_range: tuple (start_date, end_date)
    - min_confidence: float
    - max_drift: float
    - evaluation_status: "complete", "incomplete", "failed"
    - min_turns/max_turns: int
    """
```

**Usage Examples:**
```python
# Filter by model and confidence
filters = {"model": ["gpt-4", "claude-3"], "min_confidence": 0.8}

# Filter by date range and status
filters = {"date_range": ("2024-01-01", "2024-12-31"), "evaluation_status": "complete"}

# Complex multi-criteria filtering
filters = {
    "model_vendor": "anthropic",
    "min_confidence": 0.7,
    "max_drift": 0.3,
    "min_turns": 3
}
```

### **Compliance Reporting Framework**
```python
# EU AI Act Report Generation
reporter = ComplianceReporter()
report = reporter.generate_ai_act_report(
    experiment_dirs=[Path("experiments/guardrail_decay")],
    output_path=Path("compliance/ai_act_2024.json"),
    time_period="90_days"
)

# NIST AI RMF Report Generation  
nist_report = reporter.generate_nist_report(
    experiment_dirs=all_experiments,
    output_path=Path("compliance/nist_rmf_2024.json")
)
```

**Report Structure:**
- **Executive Summary**: Overall compliance score, key findings
- **Framework Sections**: Article/function-specific analysis
- **Risk Assessment**: Categorized incidents with mitigation status
- **Recommendations**: Automated suggestions for improvement

## ðŸ“Š Evaluation Models

### **Current Default**: `gemini-2.0-flash-lite`
- **Cost**: ~90% cheaper than Claude/GPT
- **Quality**: 94.9% average confidence scores
- **Speed**: Fast processing for batch operations

### **Alternative Models**
- `claude-3-7-sonnet-20250219` (higher quality, higher cost)
- `gpt-4.1` (research-grade evaluation)

## ðŸŽ¯ PHASE 3 Quality Metrics

### **Enhanced Method Agreement**
- **Statistical Testing**: Chi-square analysis for significance
- **Reliability Grading**: A-F scoring system with weighted factors
- **Disagreement Analysis**: Pattern detection and thresholds
- **Edge Case Identification**: Automated flagging for human review

### **Advanced Confidence Analysis**
- **Quality Categories**: High (â‰¥0.8), Medium (0.6-0.8), Low (<0.6)
- **Performance Indicators**: Confidence-based reliability assessment
- **Uncertainty Quantification**: Statistical confidence in evaluations
- **Calibration Tracking**: Method reliability over time

## ðŸ”§ Configuration

### **Evaluation Prompts**
- Structured JSON response format
- Clear scoring criteria (1-3 refusal scale)
- Context-aware assessment instructions

### **Rate Limiting**
- Async processing with controlled concurrency
- Vendor-specific rate limits respected
- Automatic retry with exponential backoff

## ðŸ“ Development Protocol

**Git Workflow**: Commit after every minor change with descriptive messages
```bash
git add -A && git commit -m "Description of change"
```

---
*Part of RED_CORE by Cassius Oldenburg*