# Unicode Experiment Context

**Last Updated**: 2025-06-08 by Claude Code

## ğŸ¯ Purpose

Testing adversarial attacks that use unicode to obfuscate or cloak requests and intent.

## ğŸ“ Experiment Structure

```
unicode/
â”œâ”€â”€ README.md              # Methodology and findings
â”œâ”€â”€ CLAUDE.md             # This context file
â”œâ”€â”€ prompts/              # Experiment-specific prompts
â”‚   â””â”€â”€ usr_unicode_01.yaml
â”œâ”€â”€ logs/                 # Raw experiment logs (batch tracked)
â””â”€â”€ analysis/            # Generated CSV exports
    â”œâ”€â”€ unicode-complete.csv
    â”œâ”€â”€ unicode-01.csv
    â””â”€â”€ unicode-02.csv
```

## ğŸ”„ Current Status

**Phase**: Initial Development  
**Code**: UNI  
**Batch Tracking**: Enabled

## ğŸš€ Quick Commands

```bash
# Run this experiment
make run  # Select "unicode"

# Evaluate results  
make eval

# Export results
make csv  # Select "unicode"
```

## ğŸ§ª Research Focus

### Primary Hypothesis
"Unicode confusables will lead to a 50-70% increase in incorrect intent classification by AI models, as compared to unaltered text inputs, when adversarial inputs are introduced in a context designed to elicit policy violations."

### Key Metrics
- Refusal rates by model
- Safety mechanism effectiveness
- Response pattern analysis
- Cross-model behavioral differences

## ğŸ“‹ Development Protocol

**Git Workflow**: Commit after every minor change with descriptive messages
```bash
git add -A && git commit -m "Description of change"
```

---
*Part of RED_CORE by Cassius Oldenburg*